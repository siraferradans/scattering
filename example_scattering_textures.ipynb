{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In this notebook we would like to exemplify how the Scattering features of an image allow to classify \n",
    "correctly textures, obtaining a better performance than other methods features obtained using \n",
    "the mean of wavelet-filtered images.\n",
    "\n",
    "As a reference we use the example at the scikit-image web page that uses Gabor filters for classifying \n",
    "three different textures. We will show, that for those 3 textures, these Gabor coefficients perform very well,\n",
    "but as we augment the number of textures, the problem get more difficult, and the Gabor coefficients start not\n",
    "performing that well. This is not the case with the Scattering features which still perform correctly \n",
    "(~90% correctly classified).\n",
    "\n",
    "The database used is a subset of the well know Broadtz texture database, which is online and well known as a\n",
    "benchmark in image classification. \n",
    "\n",
    "The Scattering features that we extract in this notebook are a simplification of the proposed features in \n",
    "\n",
    "Mallat, S. 'Scattering transform'\n",
    "\n",
    "More specifically, this coefficients are just the first layer of the Scattering transform. \n",
    "For more complicated classification \n",
    "problems, such as general image classification (CIFAR, or ImageNet), more advanced features are needed.\n",
    "For these cases, Scattering features have shown to be useful, as reported in:\n",
    "    \n",
    "    - Bruna, Mallat. 'Image Classification'\n",
    "    - Oyallon, Mallat. 'More image classification'\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "from scipy import ndimage as ndi\n",
    "import skimage \n",
    "from skimage import data\n",
    "from skimage.util import img_as_float\n",
    "from skimage.filters import gabor_kernel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Similarly to the example in scikit-image for Gabor filters, here we construct a Gabor filterbank\n",
    "# with J scales and L angles\n",
    "def create_gabor_filterbank(J=4,L=8):\n",
    "    kernels =[]\n",
    "    for scale in 2 ** np.arange(J) * .1:\n",
    "        filter_scale = []\n",
    "        kernels.append(filter_scale)\n",
    "        for theta in np.arange(L) / float(L) * np.pi:\n",
    "            gabor = gabor_kernel(scale, theta=theta)\n",
    "            filter_scale.append(gabor)\n",
    "    return kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#We create the set of Gabor filters\n",
    "filters = create_gabor_filterbank(J=6,L=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Following the exemple, we load just three textures \n",
    "#extract exemplar data\n",
    "\n",
    "from sklearn.feature_extraction.image import extract_patches_2d\n",
    "\n",
    "shrink = (slice(0, None, 1), slice(0, None, 1))\n",
    "image_names = ('brick', 'grass', 'wall')\n",
    "brick = img_as_float(data.load('brick.png'))[shrink]\n",
    "grass = img_as_float(data.load('grass.png'))[shrink]\n",
    "wall = img_as_float(data.load('rough-wall.png'))[shrink]\n",
    "\n",
    "images = (brick, grass, wall)\n",
    "\n",
    "px = 64\n",
    "\n",
    "brick_patches, grass_patches,wall_patches=(extract_patches_2d(image,[px, px],max_patches=100) for image in images)\n",
    "\n",
    "all_patches = np.concatenate([brick_patches, grass_patches,wall_patches],axis=0)\n",
    "all_labels = np.arange(300)//100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#This function computes the 'Gabor-like' features presented in the examples of the scikit-image web page. \n",
    "def compute_features_example(images, filters):\n",
    "    #note that the filters have different sizes, thus easier to apply the con\n",
    "    # in the spatial domain\n",
    "\n",
    "    feats = np.zeros((len(images), len(filters), len(filters[0]),2), dtype=np.float32)\n",
    "    for i,image in enumerate(images):\n",
    "        image_features=feats[i] #pointer\n",
    "        for scale,scale_output in zip(filters,image_features):\n",
    "            for kernel,kernel_l_output in zip(scale,scale_output):\n",
    "                filtered = ndi.convolve(image, kernel, mode='wrap')\n",
    "                kernel_l_output[:] = filtered.mean(),filtered.var()\n",
    "\n",
    "    return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#This function computes the first layer of the scattering transform, for each image. This amounts to compute: \n",
    "# -convolve the image with the gabor filters (psi): u=conv(x,psi)\n",
    "# -apply a non-linearity, more specifically, the modulus:  v=abs(u)\n",
    "# -mean and variance of the values: output= mean(v), mean(v^2)\n",
    "# The second layer of the scattering transform applies again the same sequence of operations. This is left as\n",
    "# future work.\n",
    "\n",
    "def compute_scattering_layer1(images, filters):\n",
    "    #note that the filters have different sizes, thus easier to apply the con\n",
    "    # in the spatial domain\n",
    "\n",
    "    feats = np.zeros((len(images), len(filters), len(filters[0]),2), dtype=np.float32)\n",
    "    for i,image in enumerate(images):\n",
    "        image_features=feats[i] #pointer\n",
    "        for scale,scale_output in zip(filters,image_features):\n",
    "            for kernel,kernel_l_output in zip(scale,scale_output):\n",
    "                \n",
    "                # |x * psi| * phi\n",
    "                filtered = np.abs(ndi.convolve(image, kernel, mode='wrap'))\n",
    "                kernel_l_output[:] = filtered.mean(), (filtered ** 2).mean()\n",
    "\n",
    "    return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ferradans/anaconda3/lib/python3.5/site-packages/numpy/core/numeric.py:474: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    }
   ],
   "source": [
    "scat_features = compute_scattering_layer1(all_patches,filters)\n",
    "ex_features = compute_features_example(all_patches,filters)\n",
    "\n",
    "scat_matrix = scat_features.reshape((len(scat_features),-1))\n",
    "ex_matrix = ex_features.reshape((len(ex_features),-1))\n",
    "patches_matrix = all_patches.reshape((len(all_patches),-1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Function that computes the classification scores, given a set of features and labels\n",
    "#using standard linear regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler, Normalizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.cross_validation import cross_val_score, KFold, ShuffleSplit\n",
    "\n",
    "def from_features_to_classif_scores(features,labels):\n",
    "    #stack the features for learning\n",
    "    features = features.reshape((len(features),-1))\n",
    "    # apply learning pipeline\n",
    "    n = len(features)\n",
    "    pipeline = make_pipeline(Normalizer(),StandardScaler(),LogisticRegression(C=1.0))\n",
    "    cv = ShuffleSplit(n,n_iter=3,test_size=1, train_size=1)\n",
    "    \n",
    "    scores = cross_val_score(pipeline,features,labels,cv=5,n_jobs=5)\n",
    "    print('score:',scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Now that we have the features, we want to see how the Gabor features and scattering coefficients\n",
    "#allow a correct classification, but not the image-patches themselves.\n",
    "print('Example initial BD:')\n",
    "from_features_to_classif_scores(ex_features,all_labels)\n",
    "\n",
    "print('Scat initial BD:')\n",
    "from_features_to_classif_scores(scat_features,all_labels)\n",
    "\n",
    "print('Patches initial BD:')\n",
    "from_features_to_classif_scores(all_patches,all_labels)\n",
    "\n",
    "print('We can see that the scattering features and the gabor-like features give perfect classification ratings')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#The previous problem is to easy to evaluate the gabor and scattering features. \n",
    "#Thus we propose to use a more complex database\n",
    "# Here, we load the new database and preprocess by extracting patches and extracting the mean\n",
    "\n",
    "import scipy.misc\n",
    "import glob\n",
    "files = glob.glob('./textures/*.tiff')\n",
    "broadtz_patches  = []\n",
    "broadtz_labels = []\n",
    "view_images = bool(1)\n",
    "if view_images: \n",
    "    fig, axes = plt.subplots(nrows=10, ncols=6, figsize=(30, 30))\n",
    "    AX = axes.ravel()\n",
    "for i,file in enumerate(files):\n",
    "    aa = data.imread(file)\n",
    "  \n",
    "    #possibly need a preprocessing (re-scaling)\n",
    "    aa = aa[:256,:256]\n",
    "    aa.shape\n",
    "    broadtz_patches.append(extract_patches_2d(aa,[px, px],max_patches=50))\n",
    "    broadtz_labels.append([i]*len(broadtz_patches[-1]))\n",
    "    if view_images :\n",
    "        AX[i].imshow(aa, cmap='gray')\n",
    "        AX[i].axis('off')\n",
    "    \n",
    "broadtz_labels = np.concatenate(broadtz_labels, axis=0)\n",
    "broadtz_patches= np.concatenate(broadtz_patches,axis=0)\n",
    "\n",
    "#take out the mean of each patch: we want to be indep. from illumination changes\n",
    "#preprocessing\n",
    "brodatz_means = broadtz_patches.reshape(len(broadtz_patches), -1).mean(1).astype('float32')\n",
    "broadtz_patches = broadtz_patches - brodatz_means[:, np.newaxis, np.newaxis]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we obtain the classification results on the new data set, using the gabor-like and scattering features. \n",
    "We observe that the scattering features outperform in almost 30% the gabor-like features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#From the patches extrated from the new database, we extract the different features\n",
    "scat_features_b=compute_scattering_layer1(broadtz_patches,filters)\n",
    "ex_features_b=compute_features_example(broadtz_patches,filters)\n",
    "\n",
    "print('Example initial BD:') \n",
    "from_features_to_classif_scores(ex_features_b,broadtz_labels)\n",
    "print('Scat initial BD:') \n",
    "from_features_to_classif_scores(scat_features_b,broadtz_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
